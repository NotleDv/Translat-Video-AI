{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c03aad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elton/Projetos/Translater_Video/jupyter/nova_implementação'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e74c1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '/home/elton/Projetos/Translater_Video/log.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa925288",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = ''\n",
    "with open(a, 'r', encoding='utf-8') as file:\n",
    "    log = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41942cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.03,\n",
       "  'end': 210.19,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/0.wav',\n",
       "  'text_origem': \" Hello all my name is Krishna and welcome to my YouTube channel. So guys in this specific video we are going to build a Generative AI application and then with the help of Docker we are going to dockerize it and deploy it in hugging face space. Now what is the Generative AI application that we are going to develop here I have actually explained so we are going to probably use We are going to develop a text generation application using LLM models and here we are also going to use transformers along with transformers. We are going to specifically use pipeline. So in transformers you have this feature of pipeline and amazing package which actually helps you to call multiple LLM models from transformers itself. So this entire process of creating this will be you know it will probably take around 20 to 25 minutes but again my main aim is to make sure to show you how we can probably containerize this entire Generative AI application with the help of Docker and finally deploy it in the hugging face space. Other than that if you also want to probably go ahead and deploy in other cloud platforms like AWS and Azure you can also do that. But the most important thing is about Dockerization how with the help of docker we can actually do that and if you don't know about docker's guys I have already created an amazing one short video. I will be putting that entire link in the description of this particular video where you can probably check it out complete one short video on docker's how to build application how to containerize it how to create an images everything is discussed over here. But most of our work right now which we are specifically working on we will try to do it in the hugging face space. Okay so step by step first of all what we will do in our first step we will go ahead and create our generative AI application in the second step we will probably create our Docker file. While we are creating this generative AI application we also need to make sure that we will be using some libraries like fast API right which actually helps us to create a end to end projects along with this will also update requirements dot TXT file right so requirements dot TXT file will also get updated. With respect to all the packages that we require okay so let's go ahead and let's build this entire plan project and again if you don't have a hugging face account please make sure that you do that and when you go to the hugging face homepage there will be an option of something called a spaces. Now inside the spaces you'll be able to see that here we will be able to create our own deployment environment and all which I will be showing you after I develop the entire project. So let's go ahead and let's start this specific project so guys now let's proceed towards implementing this entire project over here so here I have actually opened a VS code first of all what we are going to basically do is that go ahead and open our terminal and once we open our terminal we have to probably go ahead and create our new environment so that we will be able to implement this specific project so first of all what I'm actually going to do I'm just going to write conduct create minus P. V and V Python double equal to here I'm actually going to use 3.9 and by default I'm going to give yes so that I proceed with the installation of all the B-sick libraries that are actually required so once this installation takes place it is probably going to take some amount of time now what I'm actually going to do over here is that let's go ahead and first of all create our requirement dot TXT file so quickly we will go ahead and create our requirement dot TXT file.\",\n",
       "  'text_destino': 'Olá a todos meu nome é Krishna e bem-vindo ao meu canal do YouTube. En'},\n",
       " {'start': 211.91,\n",
       "  'end': 468.06,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/1.wav',\n",
       "  'text_origem': \" So requirements dot TXT file. Okay. Now in this requirement dot TXT file, I'm going to update all the requirements that I specifically require. So first of all, we require fast API, we require request, we require UBCON. UBCON is basically required in order to run your framework within the with the help of fast API library. Then we have the sentence based along with that, we will also be using torch because the generative AI application that we are specifically going to use, you know, the models that we are going to use require torch along with that transformer is another library. Inside transformer, we also have a package which is called as pipeline, which will actually help you to call any models that are available in the hugging phase. Okay. So here you can see that my environment has been created. Now what I can actually do is that go ahead and proceed with the installation of this, all the libraries. So in order to do that, okay. In order to do that, pip install, just go ahead and write minus R requirement dot TXT. Okay. Now see, how we are specifically doing this, right? Okay. I do not save the requirement of TXT in over ease. So let me just go ahead and proceed with the installation again. Now let this installation take place, right? Inside my V and V environment, all the libraries will get installed. But when we are deploying in hugging phase spaces, right? As soon as we upload this requirement of TXT along with our main file, that is app.py, right? Here automatically the installation of requirement of TXT has to be done, right? And for that, we are also going to create another Docker file. So here is what we are going to use this. So I have also created a Docker file over here. We will go ahead and update like how step-by-step, how we are going to probably create a container for this entire application. And how step-by-step, what all execution needs to happen, that all will be mentioned in the Docker file. So in short, we are going to create a package in this Docker file. So one by one, we will go ahead with. Now, since our fast API is guessing, you know, installed, right? So first of all, what I'm actually going to do is that let's go proceed and let's go ahead and start writing our code. So as I said, I'm going to create a generative AI application for TXT generation, okay? It can be a text to text generation, right? You can also use any other models that you want. So for fast API, so I'm going to import from fast API import. And here I'm going to basically use fast API, right? So this is the first library that I'm actually going to use since I've already put up in the requirement of TXT. Then from transformers, transformers, I'm going to import pipeline, right? We're also going to use the pipeline library, right? So transformers, spelling is wrong, so that is the reason I think it is giving us some errors. So from transformers, import transformers pipeline, we are going to proceed, we have to use this pipeline itself, right? So that we'll be able to call any LLM models that are presented, Hanging Face, okay? And now what I'm actually going to do, I'm going to create a new fast API, API, right? App instance, okay? And here I'm going to probably create my app, which I am going to write it as fast API, okay? So this is what I've actually done. Now let me go ahead and create my text generation pipeline. So here I'm just going to go ahead and create my text generation pipeline. And for that I will create a variable called as Pipe, and I will initialize this pipeline. And here I'm going to use some different models. So one of the most common model that you can probably find it out and it requires less space because a Hanging Face space provides you free resources to deploy the entire application. But it will be limited for, you know, from I think the CPU will be limited over there, the hard disk will be limited, the LAM will be, RAM will be limited. So we will try to select Google, let's say I will go ahead with this specific model, which is called as Flan, Flan T5, T5 small. I think T5 small will also be there. Let's see, Flan T5 small.\",\n",
       "  'text_destino': 'Então requisitos ponto arquivo TXT. Ok. Agora neste requisito ponto arquivo TXT'},\n",
       " {'start': 469.59,\n",
       "  'end': 701.52,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/2.wav',\n",
       "  'text_origem': \" Okay, so flanty 5 small we will go ahead with this model because this model again it will be requiring less space and again if you see this you'll be able to use this model with the help of transformer all you have to go ahead and write this code or you can also probably start with this pipeline right. So with the help of pipeline also you'll be able to call it and that is what I am planning to do it you know. So I will go ahead and probably copy this okay and I am going to use this specific model text to text generation along with the pipelines. So same code I will be pasting it over here and you can also do it from your side you know just copy it or use any type of model that you want okay. Now I am going to probably go ahead and create my first route which will be my home page in my first API and let's say I am going to basically go ahead and write so this will basically be my home directory right and let's return this particular message saying that hello world message is hello world something okay I am just displaying some return message for the home home page so that I will be able to see that whether everything is working fine. Now I am going to generate a function to handle some get request okay and this will be like slash generate okay. Now here what I am actually going to do is that I am going to create my app.get route okay get basically means your first and here I am going to be basically write generate okay. So get is just for taking request right it is not post so here I am going to probably create this particular route and let me just go ahead and create my generate function so here I am going to use my generate and this text will be STR so whatever text I give over here it should be able to do the post to my pipe message okay and this suggestion that you will be able to see that I am actually getting it from GitHub Copilot or okay I recently installed Amazon Q whisper and that is how I am able to get it okay. Now the next step what I am actually going to do over here is that I am going to use the pipeline to generate text from given input text okay input text so I have done that and this will basically be my output which will be assigned to my pipe right the pipe that I have actually defined the pipeline okay and this will be given the text that we have given over here okay. So once we get the text after that we can return the generated text in JSON okay JSON response and this is how simple it is with the help of hugging face transformers right and here I am going to basically return so this will basically be return whatever is my output so that I will give it in the form of key output colon and from the output we will be getting this particular response that we saw right it will be coming in the form of list and over there if you go ahead and see the documentation of Google Flan T5 small it gives you the output in a key variable which is called a generated underscore text okay. Now once you have actually done this so here what we have done is that we have created our homepage we have created our slash generate and the best thing about fast API is that it provides you it provides you a swagger documentation which we will discuss about once we run this entire application in the cloud okay. So till here I hope everybody may have understood it and we have seen that how things are basically created and with the help of fast API we are also able to see that how a simple lm application has been generated which will be able to perform text to text generation okay.\",\n",
       "  'text_destino': 'Ok, então flanty 5 pequeno vamos seguir em frente com este modelo, porque este modelo nova'},\n",
       " {'start': 700.21,\n",
       "  'end': 749.03,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/3.wav',\n",
       "  'text_origem': \" Okay, now let's go back to the Docker 5. Now this is the most important thing, guys, because here, uh, inside the Docker file, we will, we will show that how we can actually dockerize this entire application. So first of all, uh, I will go ahead and write, use the official, okay, since we have also created Python 3.9, right? Python 3.9 image. So for this, uh, what is the command that is used in Docker? You have to write from Python 3.9. And if you don't know about Docker's guys, again, I'm telling you, I've created a one short video in my channel. The link will be present in the description and even in the pinned comment of this particular video, okay? Then you can also see set the working directory.\",\n",
       "  'text_destino': 'Ok, agora vamos voltar ao Docker 5. Agora essa é a coisa mais importante,'},\n",
       " {'start': 741.82,\n",
       "  'end': 742.06,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/4.wav',\n",
       "  'text_origem': ' Okay.',\n",
       "  'text_destino': '- Está bem.'},\n",
       " {'start': 750.43,\n",
       "  'end': 791.3,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/5.wav',\n",
       "  'text_origem': ' to slash code. And I will just try it. I will just go ahead and write work directory and my directory path. So directory path will be slash code inside this. I want to probably create my working directory. And then the next step will be that copy the copy the current directory contents in the container. In the container at slash code, the slash code path that I have actually given. So for this we use a copy command. So here I will just go ahead and write the requirement or requirements.',\n",
       "  'text_destino': 'Eu vou apenas tentar. Eu vou apenas ir em frente e escrever diretório de trabalho e'},\n",
       " {'start': 793.12,\n",
       "  'end': 1078.16,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/6.wav',\n",
       "  'text_origem': \" dot txt and this will basically get copied to code slash requirements dot txt. Okay. So first of all, we require this right requirement dot txt. Here, what we are going to do is that because this requirement dot txt has all the packages. And I think the spelling is correct requirement dot txt. Okay. It has all the packages and here you'll be able to see that right. Right. Then we will go ahead and install the requirements dot txt. So this is the next step. We have to probably go ahead and do this because unless I until the requirement dot txt does not get installed, it will be of no use. And this will basically be installed from this particular code environment, right. Code code code folder. So here we are writing pip install with no cache directory upgrade minus our requirement dot txts how we install in our local right similarly pip install minus our requirement dot txt. But along with this, we are also setting up some parameters like no cache and all right. Then see, since you know that when this image is basically created and once we host it, right, it is also important that we go ahead and create a user, right. So here we set up a new user named user like run add user user, user switch the user to user user, right. This particular user we are just switching it. Okay. And it is good to do this because since we are executing in a separate environment, right. And then finally, we also go ahead and set the home to user home directory. Like initially it was slash home slash user. Now we are going to probably set this particular path. Okay. So environment variable we are setting it completely. This is how you basically set the environment variable. And now we are going to update our working directory because inside this slash home slash user, your app will be also present, right. So this is my working directory, the new working directory that we have actually set it up. And this is nothing but the user home directory, right. Then one more step we really need to do is that we need to copy the current directory contents into the home app folder, right. So that is what we are going to do over here. And let me just go ahead and use this particular command. And again, if you have familiar with docker's, I hope or Linux little bit of Linux will be able to understand it. So here we are copying all the current directories. This is this dot is with respect to the current directories into the container at home slash app. Okay. Then finally, in order to run the first up app, right. Start the sorry, first API app. Okay. And here I'm going to use the port 7680 or 7860 or you can use any port that you want. Okay. So for first API, you know that what are the dependencies that you require? We are going to create a command prompt. You become app colon app app colon app basically indicates that app is my file name and this is where your execution basically starts. You are entering your creating an instance of fast API, right. So that is the reason we are created away a host and this is 0.0.0 where it needs to probably is a local host in whatever cloud environment you basically run. And there we are going to run in the port number 7680. Okay. So this is my entire docker file and here we have actually created it. Now the next step is that I can see I already have something called as docker desktop. Okay. So if I go ahead and show you docker desktop. Okay. Now I can build this entire application with the help of docker's. I can create an image over here itself. And along with that image, I can install that particular image and for that everything you require a docker desktop. So if you go ahead and search for docker desktop, right. So with the help of docker desktop, you will be able to do it, right. And you have for windows, Mac and Linux, right. But the only requirement with respect to using docker desktop is that you at least need to have windows 10, right. And then you will be able to you can see it is out of box content. I should say offering development and team offer about hybrid tool kit to build share and run application anyway, right. So all these things are there. But see I have already shown you if you want to see my detailed one shot video, I have shown you how to install this, how to start with this and all. But here what you are going to do is that we are going to probably create our new space and we will do all the work over here, right. So let me just go ahead and write text to text with docker's. Okay. So this is my application. Here I am having an option to select docker. Okay.\",\n",
       "  'text_destino': 'Aqui, o que vamos fazer é que, porque esse requisito ponto txt tem todos os paco'},\n",
       " {'start': 1041.72,\n",
       "  'end': 1041.84,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/7.wav',\n",
       "  'text_origem': 'erro',\n",
       "  'text_destino': 'erro'},\n",
       " {'start': 1078.17,\n",
       "  'end': 1244.65,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/8.wav',\n",
       "  'text_origem': \" Right now so many different templates are supported by this in the hugging phase. So here you have Jupyter Lab, here you have InStack Auto Train and all. So I will be keeping it blank. Now as I said, hugging phase space provides you 16 GB RAM free and 2 V CPU. This is the basic one. You also have 8 V CPU with 32 GB but for this you will be required some charges. And here I am going to go with this free one just to show you how you can actually do it. And finally we go ahead and create the space. Let's see if any, okay I have to use any license. So let me just go ahead and select any license as such. So MIT license and now let me go ahead and okay so I don't need to probably provide spaces. So remove all the spaces over here and now let's click on create space. Now once this is done you can see you can clone this over here everything is there. And right now within this particular application the files is empty right. And that is what I'm actually going to do right. So I'm going to open this entire file. Let's say reveal in file explorer and this are all my files that I want right. These are all my files. I require one app.py, Docker.requiem.txt. Now what I will do I will upload it over here. So let me just go ahead and upload it and let me copy and paste it over here. So once I upload it now you see what will happen right because already in the Docker I have given all the commands that needs to get followed right all this particular commands right. So as soon as I probably put this Docker file over there and since this is already supporting that Docker environment automatically this file should run okay. So now let me just go ahead and click on commit to changes. Now as soon as this is uploaded I will go ahead and click on app. Now here you can see my entire build has actually started see all of all docker's user is adding see all the commands with respect to the Docker is getting executed one by one right. And here it is right now pushing the images exporting cache and now it goes to the container right. And now this container is going to probably run run run all these things. So no module name transformers. Let me just go ahead and see whether I have updated transformers okay and I think I have done a small spelling mistake over here. Let me just go ahead and see app.py. Yes. So it should be transformers. So I had done some spelling mistake but it's okay okay. Now what I'm actually going to do again go ahead and update this app.py. So let me quickly upload the file and again app. Okay I don't want all the three files so I'll just keep this app.py okay.\",\n",
       "  'text_destino': 'Agora, muitos modelos diferentes são suportados por isso na fase de abraço. Então, aqui'},\n",
       " {'start': 1245.98,\n",
       "  'end': 1384.29,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/9.wav',\n",
       "  'text_origem': \" Now as soon as I upload any of the file or I commit any changes, right, automatically this build process will start. See this automatically the build process will start and that is how beautiful it is. Right. Just imagine I've created so many videos on generative AI applications. Now you know, if you have even paid version with respect to hugging face and all, you can do all these things very much easily. Right. Now, as you all know that my homepage should get launched. So I should be able to get some kind of response as if this gets executed perfectly. That is in my app.py at least I should be getting this message. Return message is called to hello world. Okay. If it executes perfectly fine. Right. So let's see how much time it is probably going to take. Right. And again, see all the steps build has happened. Everything has happened. We have pushed the image over here automatically. This is doing now. This has run. See run. And this is where you are getting it. Here you are getting this hello world. Perfect. Right. So that basically means something is working. Okay. Now the next thing is that how do I see the swagger part right swagger that is this slash generate and all. And for that I obviously require some kind of URL. So what I'm going to do. I'm going to click. See in this corner right you have something over here. Right. So here you have below the settings you have some button which you called embed this space. Now once you do this you can either use this as a frame. So right now this entire page is basically getting used as a film. But you want a direct URL you can click on this right. So here is my direct URL right and here you're getting message hello world. Okay. Now let me do one thing. Let me just go ahead and write slash docs. If I go ahead and slide slash docs. So fast API provide this entire swagger UI documentation which will be able to show you in this way right. All the APIs like what all APIs are there. Next slash home is there right slash generate slash generate is what right. If you remember what we developed in slash generate we are just going to give this particular text and this is going to probably generate the output based on the pipeline that we have created from this particular model right. So this is my entire generate application. Now if you want to try it out right. So just click on try it out and just write. Hey. But tell me.\",\n",
       "  'text_destino': 'Agora, assim que eu carregar qualquer um dos arquivos ou eu cometer quaisquer'},\n",
       " {'start': 1385.96,\n",
       "  'end': 1440.36,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/10.wav',\n",
       "  'text_origem': \" tell me about machine learning. Okay, and this is done. I will just go ahead and execute it. Now here you should see this curl is going with all this response and output, a machine learning algorithm is basically coming up, right? So here you can probably see a simple output. Yes, we can go ahead and do the settings with respect to this. Okay, so this is text to text generation, right? I'm giving a text and all. Okay. So this is the text. Honesty is something. I'll just go ahead and write this. Let's see what honesty is a virtue. So it is completing the entire sentence. So in short, we are doing the text generation, right? And here you can see when I just write two words, it is being able to give me all the other words also. Okay. Let's say I go ahead and write more things. I can try multiple things over here. How to be happy. Okay.\",\n",
       "  'text_destino': 'Ok, e isso está feito. Vou apenas prosseguir e executá-lo. Agora'},\n",
       " {'start': 1443.1,\n",
       "  'end': 1496.08,\n",
       "  'path_audio': '/tmp/tmprwzk_fwp/audio/split/11.wav',\n",
       "  'text_origem': \" how to be a happy person. See, the sentence is guessing completed. And this is most like a text generation concept, where it is trying to complete the entire sentence. And that is what is the power of Google Flan T5. If you probably go ahead and see multiple examples, and this is completely based on transformers. And here you can see how beautifully we are able to run this using Swagger UI with the help of Fast API API. And we are able to see that, yes, now everything is deployed over here. So yes, I hope you like this particular video. I will also be giving you the link of the Fast API over here, Hanging Face, space, my link in the description of this particular video. Then that go ahead and write out. And yes, definitely keep on using Docker's. Right now, every company is demanding the usage of Docker's et cetera. So yes, this was it. I will see you all in the next video. Thank you. Take care. Have a great day. Bye-bye.\",\n",
       "  'text_destino': 'Veja, a frase está adivinhando completa. E isso é mais como um conceito de'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11198db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '     asfasfajkf asfjasod oaisjdfasijdf       '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84ea9413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     asfasfajkf asfjasod oaisjdfasijdf      '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118de5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0766b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "asdfaf\n",
      "1asdf\n",
      "2asdasdf\n",
      "4asdfas\n"
     ]
    }
   ],
   "source": [
    "kk = 'asdfaf 1asdf 2asdasdf 4asdfas'\n",
    "\n",
    "kk = kk.split(' ')\n",
    "print(type(kk))\n",
    "for i in kk:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e9790",
   "metadata": {},
   "source": [
    "Tentativa na raça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "tamanho_max = 50\n",
    "margem = 22\n",
    "list_temp = []\n",
    "\n",
    "for i in log[1]:\n",
    "    text = (i['text_origem'])\n",
    "    len_text = len(text)\n",
    "   \n",
    "    if text[0] == ' ':\n",
    "        for i in range(1, len_text):\n",
    "            if text[i] != ' ':\n",
    "                text = text[i:]\n",
    "                break\n",
    "            \n",
    "    if text[-1] == ' ':\n",
    "        for i in range(1,len_text):\n",
    "            if text[-i] != ' ':\n",
    "                text = text[:-i]\n",
    "                break\n",
    "    \n",
    "    new_text = text.split('. ')\n",
    "    jj = new_text\n",
    "    # bb = [new_text[1]]\n",
    "    # new_text = [new_text[1]]\n",
    "\n",
    "    for text_ in new_text:\n",
    "        #print(text_)\n",
    "        \n",
    "        len_max = tamanho_max\n",
    "        start = 0  # início do texto\n",
    "\n",
    "        if len(text_) > len_max:\n",
    "            while True:\n",
    "                if len_max >= len(text_):\n",
    "                    list_temp.append(text_[start:])  # pega o resto do texto\n",
    "                    break\n",
    "\n",
    "                if text_[len_max] == ' ':\n",
    "                    list_temp.append(text_[start:len_max])\n",
    "                    start = len_max + 1  # pula o espaço\n",
    "                    len_max += tamanho_max\n",
    "                    \n",
    "                else:\n",
    "                    for i in range(1, margem):\n",
    "                        if len_max + i >= len(text_):\n",
    "                            list_temp.append(text_[start:])\n",
    "                            len_max = len(text_)\n",
    "                            start = len_max\n",
    "                            break\n",
    "\n",
    "                        if text_[len_max + i] == ' ':\n",
    "                            list_temp.append(text_[start:len_max + i])\n",
    "                            start = len_max + i + 1\n",
    "                            len_max += tamanho_max + i\n",
    "                            break\n",
    "                    else:\n",
    "                        # fallback se não achar espaço\n",
    "                        list_temp.append(text_[start:len_max])\n",
    "                        start = len_max\n",
    "                        len_max += tamanho_max\n",
    "        else:\n",
    "            list_temp.append(text_)\n",
    "    \n",
    "    #text = list_temp\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "921e8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "beb1d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_text(text, max_len):\n",
    "    list_temp = []\n",
    "    len_text = len(text)\n",
    "    \n",
    "    if text[0] == ' ':\n",
    "        for i in range(1, len_text):\n",
    "            if text[i] != ' ':\n",
    "                text = text[i:]\n",
    "                break\n",
    "            \n",
    "    if text[-1] == ' ':\n",
    "        for i in range(1,len_text):\n",
    "            if text[-i] != ' ':\n",
    "                text = text[:-i]\n",
    "                break\n",
    "    \n",
    "    new_text = text.split('. ')\n",
    "    new_text = ' '.join(new_text)\n",
    "    \n",
    "    list_temp.append(\n",
    "            textwrap.wrap(new_text, width=max_len, break_long_words=False)\n",
    "        )\n",
    "    \n",
    "    return list_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2afa13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, element in enumerate(log[1]):\n",
    "    text = (element['text_origem'])\n",
    "    result = tratamento_text(text, 50)\n",
    "    log[1][index]['text_origem'] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fa5e0615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to slash code And I will just try it I will just',\n",
       " 'go ahead and write work directory and my directory',\n",
       " 'path So directory path will be slash code inside',\n",
       " 'this I want to probably create my working',\n",
       " 'directory And then the next step will be that copy',\n",
       " 'the copy the current directory contents in the',\n",
       " 'container In the container at slash code, the',\n",
       " 'slash code path that I have actually given So for',\n",
       " 'this we use a copy command So here I will just go',\n",
       " 'ahead and write the requirement or requirements.']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[1][5]['text_origem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bf1d10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json (path):\n",
    "    log = ''\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        log = json.load(file)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c03a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32fefb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elton/Projetos/Translater_Video/jupyter/nova_implementação\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.path.dirname(os.path.realpath('__file__')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca94bfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elton/Projetos/Translater_Video/jupyter/nova_implementação'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf639e2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models import build_model\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL = build_model('kokoro-v0_19.pth', device)\n",
    "VOICEPACK = torch.load('voices/af_heart.pt', weights_only=True).to(device)\n",
    "\n",
    "from kokoro import generate\n",
    "audio, _ = generate(MODEL, \"texto de exemplo\", VOICEPACK, lang='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d7c88398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import logging\n",
    "\n",
    "def translat (texts:list) -> str:\n",
    "    \n",
    "    #device_ = torch_config()\n",
    "    ling_input = 'en'  # <<--- Para futuras atualizações\n",
    "    ling_output = 'pt' # <<--- Para futuras atualizações\n",
    "    \n",
    "    logger = logging.getLogger('M2M100ForConditionalGeneration') \n",
    "    logger = logging.getLogger('M2M100Tokenizer') \n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to('cuda')\n",
    "    \n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "    \n",
    "    tokenizer.src_lang = ling_input # Idioma de origem\n",
    "    \n",
    "    text_completo = ''\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_hi = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(ling_output)) # Idioma Destino\n",
    "        traducao = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        text_completo += traducao[0] + ' '\n",
    "        \n",
    "    return text_completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "db738828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path_log:str):\n",
    "    log = read_json(path_log)\n",
    "    \n",
    "    q = len(log[1])\n",
    "    #bar = tqdm(total=q, desc=\"Traduzindo texto...\", colour=\"#2196F3\")\n",
    "    bar_count = 0\n",
    "    text_temp = ''\n",
    "    print(\">>>>> Inicializando a tradução\")\n",
    "    for index, texts in (enumerate(log[1])):\n",
    "        \n",
    "        result = translat(texts['text_origem'])\n",
    "        log[1][index]['text_destino'] = result\n",
    "        print('\\n\\n')\n",
    "        #bar.update(1)\n",
    "        break\n",
    "    print(log[1][0]['text_destino'])\n",
    "    print()\n",
    "    #write_json(path_log, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dede8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Inicializando a tradução\n",
      "\n",
      "\n",
      "\n",
      "Olá, meu nome é Krishna e bem-vindo ao meu canal do YouTube, por exemplo, neste vídeo Eles vão construir um aplicativo genérico e Então, com a ajuda do Docker, vamos dockerize e implante-o no espaço de cobertura do rosto Agora, o que é a Aplicação Generativa AI que nós que vai se desenvolver aqui eu realmente tenho explicado, então vamos provavelmente usar Nós somos Desenvolver uma aplicação de geração de texto Usando modelos de LLM e aqui também vamos usar Os transformadores, juntamente com os transformadores, para utilizar especificamente a pipeline Assim em transformadores Você tem essa característica de pipeline e incrível Um pacote que realmente ajuda você a chamar múltiplos Modelos de LLM de transformadores por si mesmos O processo de criar isso será você saberá que será Provavelmente levará cerca de 20 a 25 minutos, mas novamente o meu O objetivo principal é assegurar-se de mostrar-lhe como podemos Provavelmente contêinerá todo este Generative AI A aplicação com a ajuda de Docker e, finalmente, colocá-lo no espaço de cobertura do rosto, além de Se você também quiser, provavelmente, ir adiante e Em outras plataformas de nuvem como AWS e Azure Você também pode fazer isso, mas a coisa mais importante É sobre Dockerization como com a ajuda de Docker Nós podemos realmente fazer isso e se você não sabe Sobre os garotos de Docker que eu já criei um Um vídeo incrível que eu vou colocar O link completo na descrição deste particular Vídeo onde você provavelmente pode verificar o completo Um vídeo curto sobre como construir o docker Aplicar como contêiner como criar Uma imagem tudo é discutido aqui, mas A maior parte do nosso trabalho atualmente que somos trabalhando especificamente, vamos tentar fazê-lo em O espaço de abraço do rosto Bem, então passo a passo primeiro de tudo o que faremos em nosso primeiro passo Vá para frente e crie nossa aplicação generativa de AI No segundo passo, provavelmente, vamos criar o nosso Docker arquivo Enquanto estamos criando este generativo A aplicação AI também precisamos para garantir que Você vai usar algumas bibliotecas como API rápido direito que realmente nos ajuda a criar um fim para o fim Além disso, os projetos também serão atualizados. requisitos do arquivo TXT direito assim requisitos O arquivo do dot TXT também será atualizado Todos os pacotes que precisamos OK, então vamos Vamos avançar e construir este projeto inteiro e Mais uma vez, se você não tiver uma conta de cobertura Por favor, certifique-se de que você faz isso e quando você vai Para a página inicial, haverá um A opção de algo chamado espaço Agora dentro do espaços que você vai ser capaz de ver que aqui vamos ser Criar nosso próprio ambiente de trabalho e Tudo o que eu vou te mostrar depois de eu desenvolver todo o projeto, então vamos avançar e vamos Comece este projeto específico, então agora vamos para a realização de todo esse projeto A partir daí, eu realmente abri um código VS Em primeiro lugar, o que vamos fazer é que vai adiante e abre o nosso terminal e uma vez que Abre o nosso terminal que provavelmente precisamos ir adiante e Criar um novo ambiente para que possamos Para implementar esse projeto, em primeiro lugar, O que eu realmente vou fazer eu estou apenas indo para Escrever conduta cria minus P V e V Python duplo Assim como aqui, eu realmente vou usar 3.9 e Default eu vou dar sim para que eu continue com a instalação de todas as bibliotecas B-sick Na verdade, isso é necessário uma vez que A instalação ocorre, provavelmente vai Tome um pouco de tempo agora o que eu realmente sou O que vamos fazer aqui é que vamos avançar e Primeiro de tudo, crie o nosso requisito do arquivo TXT Assim, rapidamente vamos avançar e criar o nosso Requisitos do arquivo TXT. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b40d7aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello all my name is Krishna and welcome to my',\n",
       " 'YouTube channel So guys in this specific video we',\n",
       " 'are going to build a Generative AI application and',\n",
       " 'then with the help of Docker we are going to',\n",
       " 'dockerize it and deploy it in hugging face space',\n",
       " 'Now what is the Generative AI application that we',\n",
       " 'are going to develop here I have actually',\n",
       " 'explained so we are going to probably use We are',\n",
       " 'going to develop a text generation application',\n",
       " 'using LLM models and here we are also going to use',\n",
       " 'transformers along with transformers We are going',\n",
       " 'to specifically use pipeline So in transformers',\n",
       " 'you have this feature of pipeline and amazing',\n",
       " 'package which actually helps you to call multiple',\n",
       " 'LLM models from transformers itself So this entire',\n",
       " 'process of creating this will be you know it will',\n",
       " 'probably take around 20 to 25 minutes but again my',\n",
       " 'main aim is to make sure to show you how we can',\n",
       " 'probably containerize this entire Generative AI',\n",
       " 'application with the help of Docker and finally',\n",
       " 'deploy it in the hugging face space Other than',\n",
       " 'that if you also want to probably go ahead and',\n",
       " 'deploy in other cloud platforms like AWS and Azure',\n",
       " 'you can also do that But the most important thing',\n",
       " 'is about Dockerization how with the help of docker',\n",
       " \"we can actually do that and if you don't know\",\n",
       " \"about docker's guys I have already created an\",\n",
       " 'amazing one short video I will be putting that',\n",
       " 'entire link in the description of this particular',\n",
       " 'video where you can probably check it out complete',\n",
       " \"one short video on docker's how to build\",\n",
       " 'application how to containerize it how to create',\n",
       " 'an images everything is discussed over here But',\n",
       " 'most of our work right now which we are',\n",
       " 'specifically working on we will try to do it in',\n",
       " 'the hugging face space Okay so step by step first',\n",
       " 'of all what we will do in our first step we will',\n",
       " 'go ahead and create our generative AI application',\n",
       " 'in the second step we will probably create our',\n",
       " 'Docker file While we are creating this generative',\n",
       " 'AI application we also need to make sure that we',\n",
       " 'will be using some libraries like fast API right',\n",
       " 'which actually helps us to create a end to end',\n",
       " 'projects along with this will also update',\n",
       " 'requirements dot TXT file right so requirements',\n",
       " 'dot TXT file will also get updated With respect to',\n",
       " \"all the packages that we require okay so let's go\",\n",
       " \"ahead and let's build this entire plan project and\",\n",
       " \"again if you don't have a hugging face account\",\n",
       " 'please make sure that you do that and when you go',\n",
       " 'to the hugging face homepage there will be an',\n",
       " 'option of something called a spaces Now inside the',\n",
       " \"spaces you'll be able to see that here we will be\",\n",
       " 'able to create our own deployment environment and',\n",
       " 'all which I will be showing you after I develop',\n",
       " \"the entire project So let's go ahead and let's\",\n",
       " \"start this specific project so guys now let's\",\n",
       " 'proceed towards implementing this entire project',\n",
       " 'over here so here I have actually opened a VS code',\n",
       " 'first of all what we are going to basically do is',\n",
       " 'that go ahead and open our terminal and once we',\n",
       " 'open our terminal we have to probably go ahead and',\n",
       " 'create our new environment so that we will be able',\n",
       " 'to implement this specific project so first of all',\n",
       " \"what I'm actually going to do I'm just going to\",\n",
       " 'write conduct create minus P V and V Python double',\n",
       " \"equal to here I'm actually going to use 3.9 and by\",\n",
       " \"default I'm going to give yes so that I proceed\",\n",
       " 'with the installation of all the B-sick libraries',\n",
       " 'that are actually required so once this',\n",
       " 'installation takes place it is probably going to',\n",
       " \"take some amount of time now what I'm actually\",\n",
       " \"going to do over here is that let's go ahead and\",\n",
       " 'first of all create our requirement dot TXT file',\n",
       " 'so quickly we will go ahead and create our',\n",
       " 'requirement dot TXT file.']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[1][0]['text_destino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "77bfe82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translat(log[1][5]['text_origem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ad2dc0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'para esmagar o código e eu vou apenas tentar Vá adiante e escreva o diretório de trabalho e o meu diretório Caminho De modo que o Caminho de Diretório será o código slash dentro É isso que eu quero, provavelmente, criar o meu trabalho diretório e, em seguida, o próximo passo será essa cópia a cópia do conteúdo do diretório atual no em container no código slash, o Caminho de código slash que eu realmente tenho dado para Nós usamos um comando de cópia, então aqui vou para adiante e escrever o requisito ou requisitos. '"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "490a9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "\n",
    "# Define o caminho correto para o arquivo de certificados TLS/SSL\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in translated:\n",
    "    print( tokenizer.decode(t, skip_special_tokens=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67526f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elton/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Flash attention 2 is not installed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0190b3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparler-tts/parler-tts-mini-multilingual-v1.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparler-tts/parler-tts-mini-multilingual-v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m description_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtext_encoder\u001b[38;5;241m.\u001b[39m_name_or_path)\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:2488\u001b[0m, in \u001b[0;36mParlerTTSForConditionalGeneration.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast initialization is currently not supported for ParlerTTSForConditionalGeneration. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to slow initialization...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[1;32m   2486\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fast_init\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/transformers/modeling_utils.py:4097\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4091\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4092\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4093\u001b[0m     )\n\u001b[1;32m   4095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   4096\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4097\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   4100\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:2356\u001b[0m, in \u001b[0;36mParlerTTSForConditionalGeneration.__init__\u001b[0;34m(self, config, text_encoder, audio_encoder, decoder)\u001b[0m\n\u001b[1;32m   2353\u001b[0m     audio_encoder \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_config(config\u001b[38;5;241m.\u001b[39maudio_encoder)\n\u001b[1;32m   2355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2356\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m text_encoder\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_encoder \u001b[38;5;241m=\u001b[39m audio_encoder\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/transformers/modeling_utils.py:1544\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1544\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:1828\u001b[0m, in \u001b[0;36mParlerTTSForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: ParlerTTSDecoderConfig):\n\u001b[1;32m   1826\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_codebooks \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_codebooks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:1747\u001b[0m, in \u001b[0;36mParlerTTSModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: ParlerTTSDecoderConfig):\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mParlerTTSDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:1371\u001b[0m, in \u001b[0;36mParlerTTSDecoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m ParlerTTSRotaryEmbedding(\n\u001b[1;32m   1366\u001b[0m         config\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[1;32m   1367\u001b[0m         max_position_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m   1368\u001b[0m         base\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_theta,\n\u001b[1;32m   1369\u001b[0m     )\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m-> 1371\u001b[0m     [ParlerTTSDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m   1372\u001b[0m )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_implementation \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_attn_implementation\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:1371\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m ParlerTTSRotaryEmbedding(\n\u001b[1;32m   1366\u001b[0m         config\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[1;32m   1367\u001b[0m         max_position_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m   1368\u001b[0m         base\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_theta,\n\u001b[1;32m   1369\u001b[0m     )\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m-> 1371\u001b[0m     [\u001b[43mParlerTTSDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m   1372\u001b[0m )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_implementation \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_attn_implementation\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/parler_tts/modeling_parler_tts.py:979\u001b[0m, in \u001b[0;36mParlerTTSDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_attn \u001b[38;5;241m=\u001b[39m PARLERTTS_ATTENTION_CLASSES[cross_attn_implementation](\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m    969\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_attn_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mffn_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_tts/lib/python3.10/site-packages/torch/nn/init.py:589\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    587\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-multilingual-v1.1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-multilingual-v1.1\")\n",
    "description_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2717073",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = r\"/wsl.localhost/Ubuntu-24.04/tmp/tmpgrmq07cl/log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e83edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Construa o modelo e carregue o pacote de voz padrão\"\n",
    "description = \"Sophia voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c9c28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = description_tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa51c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "#from app.configs import read_json, write_json, torch_config\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3809640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_audio (text, path_destino, index):\n",
    "    #device = torch_config()\n",
    "    device = 'cuda'\n",
    "    model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-multilingual-v1.1\").to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-multilingual-v1.1\")\n",
    "    description_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\n",
    "    \n",
    "    contador = 0\n",
    "\n",
    "    prompt = text\n",
    "    description = \"Sophia voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\n",
    "    \n",
    "    path_audios_gerados = []\n",
    "    name_audio = os.path.join(path_destino, f'audio{index}.wav')    \n",
    "    path_audios_gerados.append({f'audio_{index}': name_audio})\n",
    "    \n",
    "    input_ids = description_tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "    sf.write(name_audio, audio_arr, model.config.sampling_rate)\n",
    "        \n",
    "    return path_audios_gerados  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136c2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json (path):\n",
    "    log = ''\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        log = json.load(file)\n",
    "    \n",
    "    return log\n",
    "\n",
    "\n",
    "def write_json (path, dados):        \n",
    "    try:\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(json.dumps(dados, ensure_ascii=False, indent=2))\n",
    "    except:\n",
    "        print('erro ao salvar o json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ad77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "def main(path_log:str):\n",
    "    log = read_json(path_log)\n",
    "    \n",
    "    output = log[0]['path_dir_audio_generator']\n",
    "    \n",
    "    q = len(log[1])\n",
    "    #bar = tqdm(total=q, desc=\"Gerando áudios...\", colour=\"#2196F3\")\n",
    "    bar_count = 0\n",
    "    \n",
    "    for index, i in enumerate(log[1]):\n",
    "        print(i)\n",
    "        gerar_audio(i['text_destino'], output, index)\n",
    "    #     bar.update(bar_count)\n",
    "        \n",
    "    #     log[1][index]['path_audios_gerados'] = gerar_audio(i['text_destino'], output, index)\n",
    "\n",
    "    #     bar_count+=1\n",
    "    \n",
    "    # print()\n",
    "    # write_json(path_log, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5dfd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elton/Projetos/Translater_Video/jupyter/nova_implementação'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5f2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = '/home/elton/Projetos/Translater_Video/log.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cfbce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 6.92, 'end': 27.96, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/0.wav', 'text_origem': ['Before we can begin, it is necessary that both', 'Cinema4T and Redshift are already installed We', \"will do this of course via the Maxon app, as I'm\", 'sure you know, you can find the Maxon app directly', 'on the Maxon website We go to products directly to', 'downloads, then you can download the app right up', 'here For Windows, Maca Linux, you can also', 'download it here below.'], 'text_destino': 'Antes de começar, é necessário que os dois Cinema4T e Redshift já estão instalados Isso será feito através da aplicação Maxon, como eu sou. Com certeza você sabe, você pode encontrar o aplicativo Maxon diretamente no site da Maxon vamos aos produtos diretamente para downloads, então você pode baixar o aplicativo diretamente para cima Aqui Para Windows, Maca Linux, você também pode Baixe aqui abaixo. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 28.52, 'end': 29.87, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/1.wav', 'text_origem': ['Then we open the app.'], 'text_destino': 'Então vamos abrir o aplicativo. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 30.9, 'end': 34.52, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/2.wav', 'text_origem': ['Then you can update Cinema4D and owns a redshift', 'to the latest versions.'], 'text_destino': 'Então você pode atualizar Cinema4D e possuir um redshift às versões mais recentes. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 35.57, 'end': 54.94, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/3.wav', 'text_origem': ['And then when you open SimFudi, it should look', 'something like this Then on the right side,', 'another command group, you will see these three', 'objects These still contain some redshift objects', 'Then you should also see redshift at the top of', \"the main menu And if you don't see either, then\", \"you probably haven't switched the renderer to\", 'redshift yet.'], 'text_destino': 'E então, quando você abre o SimFudi, ele deve olhar O que é assim, no lado direito, Outro grupo de comando, você verá estes três Objetos Estes ainda contêm alguns objetos redshift Então você também deve ver a mudança vermelha no topo da O menu principal E se você também não vê, então Você provavelmente não mudou o render para Redshift ainda. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 50.39, 'end': 50.71, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/4.wav', 'text_origem': ['Alone!'], 'text_destino': 'sozinho! '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 55.79, 'end': 73.79, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/5.wav', 'text_origem': [\"It's very easy You go to Edit Render Settings,\", 'then you can switch it back to Standard And if the', 'whole thing is set to Standard, you can see here', 'it looks like this And now we have here the', 'default objects of Cinema4D The menu appears', 'completely gone Let me put this back.'], 'text_destino': 'É muito fácil você ir para Edit Render Settings, Então você pode trocá-lo para o padrão e se o Tudo está configurado para o padrão, você pode ver aqui Parece assim e agora temos aqui o Objetos padrão do Cinema4D O menu aparece Completamente desaparecido Deixe-me colocar isso de volta. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 75.16, 'end': 85.89, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/6.wav', 'text_origem': ['It is possible that you have switched the run-roid', \"redshift and still don't see a menu from redshift\", 'In this case, you can go to the menu to edit, then', 'preferences.'], 'text_destino': 'É possível que você tenha trocado o run-roid Redshift e ainda não vê um menu do Redshift Neste caso, você pode ir ao menu para editar, então das preferências. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 88.59, 'end': 106.28, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/7.wav', 'text_origem': ['Navigates to the renderer section, opens the menu', 'and select RETCHIFT and go to the user interface', 'and there you have a checkbox for RETCHIFMain menu', 'If I uncheck this, the menu disappears for me and', 'activated again, the RETCHIFMain menu reappears.'], 'text_destino': 'Navegue para a seção de render, abre o menu e selecione RETCHIFT e vá para a interface de usuário e aí você tem uma caixa de verificação para o menu RETCHIFMain Se eu descuidar isso, o menu desaparece para mim e Reativado, o menu RETCHIFMain reaparece. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 108.86, 'end': 110.95, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/8.wav', 'text_origem': ['Now we need something to render.'], 'text_destino': 'Agora precisamos de algo para entregar. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 113.09, 'end': 125.56, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/9.wav', 'text_origem': ['I suggest we create a very simple scene I would', 'just use the scene of this asset browser for that', 'In the search bar, I type in Backdrop.'], 'text_destino': 'Eu sugiro que criemos uma cena muito simples que eu queria Use apenas a cena deste navegador de ativos para que Na barra de pesquisa, digite em Backdrop. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 127.42, 'end': 138.68, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/10.wav', 'text_origem': ['The BackDock object is a fillet, here is our', 'fillet I also have the list view displayed, so you', 'can see this better The name is StudioBackdrop and', 'you can also see the redshift logo by the object.'], 'text_destino': 'O objeto BackDock é um filete, aqui está o nosso Eu também tenho a vista da lista exibida, então você Pode ver isso melhor O nome é StudioBackdrop e Você também pode ver o logotipo redshift pelo objeto. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 139.44, 'end': 145.85, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/11.wav', 'text_origem': ['From this we can conclude that this object', 'contains a retrof matured and I double click on it', \"Here's our parametric fillet.\"], 'text_destino': 'Deste modo, podemos concluir que este objeto contém um retrof amadurecido e eu clique duas vezes nele Aqui está o nosso parâmetro. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 147.03, 'end': 159.74, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/12.wav', 'text_origem': ['Now if I click on the object then I have some', 'parameters here that I can adjust Here you can', 'adjust the fillet as needed, then replace an', 'object on the fillet In the search bar, I retire,', 'redshift.'], 'text_destino': 'Agora, se eu clicar no objeto, então eu tenho alguns Parâmetros aqui que eu posso ajustar Aqui você pode ajustar o filete conforme necessário, em seguida, substituir um Objeto no filete Na barra de busca, eu me aposento, em redshift. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 162.1, 'end': 165.61, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/13.wav', 'text_origem': ['Here are all the models with associated redshift', 'materials.'], 'text_destino': 'Aqui estão todos os modelos com redshift associados dos materiais. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 168.97, 'end': 191.6, 'path_audio': '/tmp/tmpgrmq07cl/audio/split/14.wav', 'text_origem': ['scrolling down further to take with, I will take a', 'chair, I choose this blue chair, you can choose', 'any object you like, of course you can also work', 'with basic objects, even a simple sphere on a', 'surface would be enough, but I will take the', 'designer chair 0.1 now And blaze it directly on it', \"and close the asset browser, and that's it for\", 'now, and now we have a basic'], 'text_destino': 'Depois de escorregar mais para levar comigo, vou tomar um Eu escolho esta cadeira azul, você pode escolher Qualquer objeto que você gosta, você também pode trabalhar com objetos básicos, mesmo uma esfera simples em um A superfície seria suficiente, mas eu vou tomar o Cadeira de design 0.1 agora e ligue-a diretamente nele e fechar o navegador de ativos, e isso é para Agora, e agora, temos uma base '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
      "  \"architectures\": [\n",
      "    \"DacModel\"\n",
      "  ],\n",
      "  \"codebook_dim\": 8,\n",
      "  \"codebook_loss_weight\": 1.0,\n",
      "  \"codebook_size\": 1024,\n",
      "  \"commitment_loss_weight\": 0.25,\n",
      "  \"decoder_hidden_size\": 1536,\n",
      "  \"downsampling_ratios\": [\n",
      "    2,\n",
      "    4,\n",
      "    8,\n",
      "    8\n",
      "  ],\n",
      "  \"encoder_hidden_size\": 64,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"hop_length\": 512,\n",
      "  \"model_type\": \"dac\",\n",
      "  \"n_codebooks\": 9,\n",
      "  \"quantizer_dropout\": 0.0,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    8,\n",
      "    4,\n",
      "    2\n",
      "  ]\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"delay_strategy\": \"delay\",\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": true,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(path_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
