{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56b3c23",
   "metadata": {},
   "source": [
    "### ðŸ› ï¸ ConfiguraÃ§Ã£o de device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf9293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "\n",
    "device_ = \"cpu\" if not n_devices else \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff9b82",
   "metadata": {},
   "source": [
    "### ðŸ—ƒï¸ Recuperando o log do notebook 01_EA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d534a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bc473eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_video = \"/home/elton/Projetos/Translater_Video/teste\"\n",
    "name_video = \"1 - Preparations.mp4\"\n",
    "\n",
    "name_audio_out = \"audio_completo.wav\"\n",
    "path_out = \"/home/elton/Projetos/Translater_Video/\"\n",
    "\n",
    "path_log = os.path.join(path_out,  'log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a6f41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mpath_log\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     log \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_log' is not defined"
     ]
    }
   ],
   "source": [
    "log = ''\n",
    "with open(path_log, 'r', encoding='utf-8') as file:\n",
    "    log = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb25f1",
   "metadata": {},
   "source": [
    "## ExtraÃ§Ã£o de Texto do Ã¡udio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ff9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c54c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversao_audio_texto (path):\n",
    "       \n",
    "    #Carregando modelo\n",
    "    model = whisper.load_model(\"turbo\");\n",
    "    \n",
    "    #Realizando a transcriÃ§Ã£o\n",
    "    result = model.transcribe(path)\n",
    "    \n",
    "    if result['text']:\n",
    "        return result\n",
    "    else:\n",
    "        return 'erro'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc702d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elton/Projetos/Translater_Video/Nova pasta/audio_completo.wav'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = os.path.join('/home/elton/Projetos/Translater_Video','Nova pasta', 'audio_completo.wav')\n",
    "a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd5d3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = conversao_audio_texto(a )\n",
    "\n",
    "\n",
    "# for index, i in enumerate(log):\n",
    "    \n",
    "#     result = conversao_audio_texto(i['path_audio'])\n",
    "#     log[index]['text_origem'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eb805ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello all my name is Krish Nayak and welcome to my YouTube channel. So guys in this specific video we are going to build a generative AI application and then with the help of Docker we are going to dockerize it and deploy it in Hugging Face space. Now what is the generative AI application that we are going to develop here I have actually explained so we are going to probably use we are going to develop a text generation application using LLM models and here we are also going to use transformers along with transformers we are going to specifically use pipeline okay so in transformers you have this feature of pipeline an amazing package which actually helps you to call multiple LLM models from transformer itself so this entire process of creating this will be you know it will probably take around 20-25 minutes but again my main aim is to make sure to show you how we can probably containerize this entire generative AI application with the help of docker and finally deploy it with in the hugging face space. Other than that if you also want to probably go ahead and deploy in other cloud platforms like AWS and Azure you can also do that right but the most important thing is about dockerization how with the help of dockers we can actually do that and if you don't know about dockers guys I have already created an amazing one-shot video I will be putting that entire link in the description of this particular video where you can probably check it out complete one short video on dockers how to build application how to container I containerize it how to create an images everything is discussed over here but most of our work right now which we are specifically working on we will try to do it in the hugging face space okay so step by step first of all what we will do in our first step we will go ahead and create our generative AI application in the second step we will probably create our docker file while we are creating this generative AI application we also need to make sure that we will be using some libraries like fast API right which actually helps us to create a end-to-end projects along with this will also update requirements dot txt file right so requirements dot txt file will also get updated with respect to all the packages that we require okay so let's go ahead and let's build this entire player project and again if you don't have a hugging face account please make sure that you do that and when you go to the hugging face home page there will be an option of something called a spaces now inside the spaces you'll be able to see that here we will be able to create our own deployment environment and all which I will be showing you after I develop the entire project so let's go ahead and let's start this specific project so guys now let's proceed towards implementing this entire project over here so here I've actually opened a VS code first of all what we are going to basically do is that go ahead and open our terminal and once we open our terminal we have to probably go ahead and create our new environment so that we will be able to implement this specific project so first of all what I'm actually going to do I'm just going to write conduct create minus p vnv python double equal to here I'm actually going to use 3.9 and by default I'm going to give yes so that I proceed with the installation of all the basic libraries that are actually required so once this installation takes place it is probably going to take some amount of time now what I'm actually going to do over here is that let's go ahead and first of all create our requirement or txt file so quickly we will go ahead and create our requirement or txt file so requirements dot txt file okay now in this requirement or txt file I'm going to update all the requirements that I specifically require so first of all we require fast api we require request we require ubicon ubicon is basically required in order to run your framework within the with the help of fast api library then we have the sentence based along with that we will also be using torch because the generative AI application that we are specifically going to use you know the models that we are going to use requires torch along with that transformer is another library inside transformer we also have a package which is called as pipeline which will actually help you to call any models that are available in the hugging phase okay so here you can see that my environment has been created now what I can actually do is that go ahead and proceed with the installation of this all the libraries so in order to do that okay in order to do that pip install just go ahead and write minus our requirement dot txt okay now see how we are specifically doing this right okay I did not save the requirement or txt no worries so let me just go ahead and proceed with the installation again now let this installation take place right inside my vnv environment all the libraries will get installed but when we are deploying in hugging face spaces right as soon as we upload this requirement dot txt along with our main file that is app dot py right here automatically the installation of requirement of txt has to be done right and for that we are also going to create another docker file so here is what we are going to use this so I have also created a docker file over here we will go ahead and go ahead and update like how step by step how we are going to probably create a container for this entire application and how step by step what all execution needs to happen that all will be mentioned the docker file so in short we are going to create a package in this docker file so one by one we will go ahead with now since our fast API is guessing you know installed right so first of all what I'm actually going to do is that let's go proceed and let's go ahead and start writing our code so as I said I'm going to create a generative AI application for text generation okay it can be a text to text generation right you can also use any other models that you want so for fast API so I'm going to import from fast API import okay and here I'm going to basically use fast API right so this is the first library that I'm actually going to use since I've already installed put up in the requirement or txt then from transformers transformers I'm going to import pipeline right we're also going to use the pipeline library site so transformers spelling is wrong so that is the reason I think it is giving us some errors so from transformers import transformer pipeline we are going to proceed we have to use this pipeline itself right so that we'll be able to call any LLM models that are presented hugging face okay and now what I'm actually going to do I'm going to go ahead and create a new fast API API right app instance okay and here I'm going to probably create my app which I am going to write it as fast API okay so this is what I've actually done now let me go ahead and create my text generation pipeline so here I'm just going to go ahead and create my text generation pipeline and for that I will create a variable called as pipe and I will initialize this pipeline and here I'm going to use some different different models so one of the most common model that you can probably find it out and it requires less space because see a hugging face space provides you free free resources you know to deploy the entire application but it will be limited for you know from I think the CPU will be limited over there the hardest will be limited the lamb will be RAM will be limited so we will try to select Google or let's say I will go ahead with this specific model which is called as flan flan t5 t5 small I think t5 small will also be there let's see flan t5 small okay so flan t5 small we will go ahead with this model because this model again it will be requiring less space and again if you see this you'll be able to use this model with the help of transformer all you have to go ahead and write this code or you can also probably start with this pipeline right so with the help of pipeline also you'll be able to call it and that is what I am planning to do it you know so I will go ahead and probably copy this okay and I'm going to use this specific model text to text generation along with the pipeline so same code I will be pasting it over here and you can also do it from your side you know just copy it or use any type of model that you want okay now I'm going to probably go ahead and create my first route which will be my home page in my fast API and let's say I'm going to basically go ahead and write so this will basically be my home directory right and let's return this particular message saying that hello world or message is hello world something okay I'm just displaying some return message for the home home page so that I'll be able to see that whether everything is working fine now I'm going to generate a function to handle some get request okay and this will be like slash generate okay now here what I'm actually going to do is that I'm going to create my app dot get route okay get basically means your first and here I'm going to basically write generate okay so get is just for taking request right it's not post so here I'm going to probably create this particular route and let me just go ahead and create my generator function so here I'm going to use my generate and this text will be str for whatever text I give over here it should be able to do the post to my pipe message okay and this suggestion that you'll be able to see that I'm actually getting it from github copilot or okay I recently installed Amazon Q whisper and that is how I am able to get it okay now the next step what I'm actually going to do over here is that I'm going to use the pipeline to generate text from given input text okay input text so I've done that and this will basically be my output which will be assigned to my pipe right the pipe that I have actually defined the pipeline okay and this will be given the text that we have given over here okay so once we get the text after that we can return the generated text in JSON okay JSON response this is how simple it is with the help of hugging face transformers right and here I'm going to basically return also this will basically be written whatever is my output so that I will give it in the form of key output colon and from the output we will be getting this particular response that we saw right it will be coming in the form of list and over there if you go ahead and see the documentation of Google flan t5 small it gives you the output in a key variable which is called a generated underscore text okay now once you have actually done this so here what we have done is that we have created our home page we have created our slash generate and the best thing about fast API is that it provides you it provides you a swagger documentation which we'll discuss about once we run this entire application in the cloud okay so till here I hope everybody may have understood it and we have seen that how things are basically created and with the help of fast API we are also able to see that how a simple LLM application has been generated which will be able to perform text to text generation okay now let's go back to the docker file now this is the most important thing guys because here inside the docker file we will we will show that how we can actually dockerize this entire application so first of all I will go ahead and write use the official okay since we have also created Python 3.9 right Python 3.9 image so for this what is the command that is used in docker you have to write from Python 3.9 and if you don't know about docker guys again I'm telling you I've created a one-shot video in my channel the link will be present in the description and in even in the pinned comment of this particular video okay then you can also see set the working directory to slash code okay and I will just write I will just go ahead and write work directory and my directory path so directory path will be slash code inside this I want to probably create my working directory and then the next step will be that copy the copy the current directory contents in the container in the container in the container at slash code the slash code path that I've actually given so for this we use a copy command so here I will just go ahead and write requirement requirements dot txt and this will basically get copied to code slash requirements dot txt okay so first of all we require this right requirement dot txt here what we are going to do is that because this requirement dot txt has all the packages and I think the spelling is correct requirement dot txt okay it has all the packages and here you'll be able to see that right then we will go ahead and install the requirements dot txt so this is the next step we have to probably go ahead and do this because unless and until the requirement dot txt does not get installed it will be of no use and this will basically be installed from this particular code environment right code code code folder so here we are writing pip install with no cache directory upgrade minus our requirement dot txt how we install in our local right similarly pip install minus our requirement dot txt but along with this we are also setting up some parameters like no cache and all right then now see since you know that when this image is basically created and once we host it right it is also important that we go ahead and create a user right so here we set up a new user named user like run add user add user switch the user to user user right so this particular user we are just switching it okay and it is good to do this uh because uh since we are executing in a separate environment right and then finally uh we also go ahead and set the home to user home directory like initially it was slash home slash user now we are going to probably set this particular path okay so environment variable we are setting it completely uh this is how you basically set the environment variable and now we are going to update our working directory because inside this slash home slash user your app will be also present right so this is my working directory the new working directory that we have actually set it up and this is nothing but the user home directory right then uh one more step we really need to do is that uh we need to copy the current directory contents into the home app folder right so that is what we are going to do over here and let me just go ahead and use this particular command and again if you are familiar with docker i hope or linux a little bit of linux will be able to understand it so here we are copying all the current directories this is this dot is with respect to the current directories into the container at home slash app okay then finally in order to run the fast app app right start the sorry fast api app okay and here i'm going to use the port 7680 or 7860 or you can use any port that you want okay so uh for fast api you know that what are the dependencies that you require we are going to create a command prompt uvicon app colon app app colon app basically indicates that uh app is my file name and this is where your execution basically starts you are entering you are creating an instance of fast api right uh so that is the reason we are created over here host and this is uh 0.0.0 where it needs to prop it is a local host in whatever cloud environment you basically run and there we are going to run in the port number 7680 okay so this is my entire docker file and here we have actually created it now the next step is that uh see i can i can see i already have something called as docker desktop okay so if i go ahead and show you docker desktop okay now i can build this entire application with the help of dockers i can create an image over here itself and along with that image i can install that particular image and for that everything you require a docker desktop so if you go ahead and search for docker desktop right so with the help of docker desktop you will be able to do it right and you have for windows mac and linux right uh but the only requirement with respect to using docker desktop is that you at least need to have windows 10 right and then you'll be able to you can see it is a out-of-box continuation software uh offering development and team of robot hybrid toolkit to build share and run application anyway right so all these things are there but see i have already shown you if you want to see my detailed one shot video i've shown you how to install this how to start with this and all but here what we are going to do is that we are going to probably create our new space and we'll do all the work over here right so let me just go ahead and write text to text with dockers okay so this is my application here i am having an option to select docker right now uh so many different different templates are supported by this right in the hugging face so here you have jupiter lab here you have aimstack auto train and all so i will be keeping it blank now as i said right hugging face space provides you 16 gb ram free and 2v cpu this is the basic one you also have 8v cpu with 32 gb but for this you will be required some charges right but here i'm going to go with this free one just to show you how you can actually do it and finally we go ahead and create the space let's see if any okay i have to use any license so let me just go ahead and select uh any license as such so mit license and now let me go ahead and okay so i don't need to probably provide spaces so remove all the spaces over here and now let's click on create space now once this is done you can see you can clone this over here everything is there and right now within this particular application the files is empty right and that is what i'm actually going to do right so i'm going to open this entire file let's say reveal in file explorer and these are all my files that i want right these are all my files i require one app.py docker.requirement.txt now what i will do i will upload it over here okay so let me just go ahead and upload it and let me copy and paste it over here right so once i upload it now you see what will happen right because already in the docker i have given all the commands that needs to get followed right all these particular commands right so as soon as i probably put this docker file over there and since this is already supporting that docker environment automatically this file should run okay so now let me just go ahead and click on commit to changes now as soon as this is uploaded i will go ahead and click on app now here you can see my entire build has actually started see all docker's user is adding see all the commands with respect to the docker is getting executed one by one right and here here it is right now pushing the images exporting cache now it goes to the container right and now this container is going to probably run all these things so no module name transformers uh let me just go ahead and see whether i have updated transformers okay and i think i've done a small spelling mistake over here let me just go ahead and see app.py yes so it should be transformers so i had done some spelling mistake but it's okay okay now what i'm actually going to do again go ahead and update this app.py so let me go quickly upload the file and again app.py okay i don't want all three files so i'll just keep this app.py okay now as soon as i upload any of the file or i commit any changes right automatically this build process will start see this automatically the build process will start and that is how beautiful it is right now just imagine i've created so many videos on generative ai applications now you know if you have even paid version with respect to logging face and all you can do all these things very much easily right now as you all know that my home page should get launched so i should be able to get some kind of response as if this gets executed perfectly that is in my app.py at least i should be getting this message return message is called to hello world okay if it executes perfectly fine right so let's see how much time it is probably going to take right uh and again see all the steps build has happened everything has happened we have pushed the image over here automatically this is doing now this has run see run and this is where you are getting it right here you are getting this hello world perfect right so that basically means something is working okay now the next thing is that how do i see the swagger part right swagger that is this slash generate and all and for that i obviously require some kind of url so what i'm going to do uh i'm going to click uh see in this corner right you have something over here right so here you have below the settings you have some button which is called embed this space now once you do this you can either use this as iframe so right now this entire page is basically getting used as iframe but you want a direct url you can click on this right so here is my direct url right and here you are getting message hello world okay now let me do one thing let me just go ahead and write slash docs if i go ahead and write slash docs so fast api provides this entire swagger ui documentation which will be able to show you in this way right all the apis like what all apis are there like slash home is there right uh slash generate now slash generate is what right if you remember what we developed in slash generate we are just going to give this particular text and this is going to probably generate the output based on the pipeline that we have created from this particular model right so this is my entire generative application now if you want to try it out right so just click on try it out and just write hey uh but tell me tell me about machine learning okay and this is done i will just go ahead and execute it now here you should see this curl is going with all this response and uh output a machine learning algorithm is basically coming up right so uh here you can probably see a simple output yes we can go ahead and do the settings with respect to this okay so this is text to text generation right i'm giving a text and all okay uh let me just honesty is something i'll just go ahead and write this let's see what honesty is a virtue so it is completing the entire sentence so in short we are doing the text generation right and here you can see when i just write two words it is being able to give me all the other words also okay um let's say i go ahead and write more things like i can try multiple things over here um how to be happy okay how to be a happy person see the sentence is getting completed right and this is most like a text generation concept uh where it is trying to complete the entire sentence and that is what is the power of google flantify right uh if you probably go ahead and see multiple examples and this is completely based on transformers right uh and here you can see how beautifully we are able to run this using swagger ui in the faster with the help of fast api api and we are able to see that yes now everything is deployed over here so yes i hope you like this particular video i will also be giving you the link of the fast api over here hugging face space my link in the description of this particular video other than that go ahead and try it out and yes definitely keep on using dockers right now every company is demanding the usage of lockers itself so yes this was it i will see you all in the next video thank you take care have a great day bye\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bd9d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_log, 'w', encoding='utf-8') as file:\n",
    "    file.write(json.dumps(log, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e35b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "236233d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig( load_in_8bit=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6339b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes estÃ¡ funcionando!\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(\"bitsandbytes estÃ¡ funcionando!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07b28f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjbochi/madlad400-3b-mt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device_)\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_core/lib/python3.10/site-packages/transformers/modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Projetos/Translater_Video/.venv_core/lib/python3.10/site-packages/transformers/modeling_utils.py:4933\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 4933\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4935\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4936\u001b[0m         )\n\u001b[1;32m   4938\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   4939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mValueError\u001b[0m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_name = 'jbochi/madlad400-3b-mt'\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.to(device_)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"<2pt> I love pizza!\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Eu adoro pizza!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab_Translat_Video",
   "language": "python",
   "name": "lab_translat_video"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
